# Research Papers 

## Focus Areas: LLMs, Generative AI, ML

| **Publication**  | **Links** |
| ------------- | ------------- |
| **Graph Machine Learning in the Era of Large Language Models (LLMs)** -  Graphs are pivotal for representing intricate relationships in diverse domains, while Graph Neural Networks (GNNs) have become fundamental in processing such structures. Recently, the surge in Large Language Models' (LLMs) capabilities has prompted exploration into their integration with graphs to advance Graph Machine Learning (Graph ML) tasks, leveraging the rich factual knowledge in graphs to enhance LLM reasoning while necessitating a systematic review to comprehend the latest advancements and future prospects in this burgeoning field.| [Publication](https://arxiv.org/abs/2404.14928)|
| **Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone** -  Phi-3-mini compact language model with 3.8 billion parameters trained on an extensive dataset of 3.3 trillion tokens, demonstrating performance comparable to larger models like Mixtral 8x7B and GPT-3.5. Notably, phi-3-mini achieves impressive scores of 69% on MMLU and 8.38 on MT-bench, all while being small enough for deployment on mobile device.| [Publication](https://arxiv.org/abs/22404.14219)|
| **Long-context LLMs Struggle with Long In-context Learning** -  The study introduces LIConBench, a specialized benchmark focusing on long in-context learning for extreme-label classification, covering datasets with label ranges from 28 to 174 classes and input lengths from 2K to 50K tokens. LIConBench requires LLMs to comprehend entire inputs to predict across massive label spaces accurately..| [Publication](https://arxiv.org/abs/2404.02060)|
| **How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs' internal prior** -  Retrieval augmented generation (RAG) is commonly employed to address errors and offer updated knowledge for large language models (LLMs). However, does providing correct retrieved content always rectify LLM errors, and can LLMs discern and disregard incorrect retrieved information? To explore this, author's analyze the interplay between LLMs' internal knowledge and retrieved data when they conflict. Testing GPT-4 and other LLMs on question-answering tasks with and without reference documents, authors find that correct retrieved information largely corrects errors. Yet, when reference documents contain incorrect data, LLMs are prone to echoing the errors, especially when their prior knowledge is weak, but resist doing so when their prior knowledge is strong. Similarly, the more the modified information differs from the model's prior, the less likely the model is to endorse it. These findings underscore the inherent tension between a model's prior knowledge and external reference documents.| [Publication](https://arxiv.org/abs/2404.10198)|
| **VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time** -  VASA is a groundbreaking framework that generates lifelike talking faces from a static image and speech audio clip, featuring VASA-1, which not only synchronizes lip movements with audio but also captures diverse facial nuances and natural head motions, achieved through innovative diffusion-based facial dynamics and head movement models, leading to superior performance in generating high-quality videos in real-time, thereby enabling engaging interactions with human-like avatars.| [Publication](https://arxiv.org/abs/2404.10667)|
| **The Unreasonable Ineffectiveness of the Deeper Layers** -  The study explores the efficacy of a layer-pruning strategy on popular large language models (LLMs) pretrained with open weights, finding that removing up to half of the layers leads to minimal performance decline across various question-answering benchmarks. By identifying the optimal block of layers for removal and employing parameter-efficient finetuning techniques like quantization and Low Rank Adapters (QLoRA), the process becomes feasible on a single GPU. These findings suggest that layer pruning can complement other parameter-efficient finetuning methods, potentially reducing computational resources during finetuning and improving memory and latency during inference, while also raising questions about the utilization of parameters in deeper layers versus the importance of shallow layers in storing knowledge within LLMs.| [Publication](https://arxiv.org/abs/2403.17887v1)|
| **Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Model** -  Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Discussed the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.| [Publication](https://arxiv.org/abs/2402.17177v2)|
| **GaLore:Memory-Efficient LLM Training** -  Training Large Language Models (LLMs) is memory-intensive due to weight and optimizer state size. Common memory-reduction methods like low-rank adaptation (LoRA) underperform as they limit parameter search and alter training dynamics.  Gradient Low-Rank Projection (GaLore), enabling full-parameter learning while reducing memory usage by up to 65.5% in optimizer states. GaLore achieves efficient pre-training and fine-tuning, even on consumer GPUs, showcasing up to 82.5% reduction in optimizer memory and 63.3% reduction in total training memory compared to a BF16 baseline.| [Publication](https://arxiv.org/abs/2403.03507)|
| **The Era of 1-bit LLMs** -  This study introduces a 1-bit LLM variant called BitNetb1.58, where every single parameter (weight) of the LLM is ternary {-1, 0, 1}. BitNetb1.58 matches the full-precision Transformer LLM in terms of model size and training tokens, achieving similar perplexity and end-task performance. However, it proves significantly more cost-effective in terms of latency, memory usage, throughput, and energy consumption. Notably, the 1.58-bit LLM establishes a new scaling law and training approach for creating high-performance and cost-effective LLMs. It also opens the possibility of a new computation paradigm and the design of hardware optimized specifically for 1-bit LLMs. | [Publication](https://arxiv.org/abs/2402.17764)|
| **AtP*** -  Activation Patching computes causal attributions of behavior to model components, but its exhaustive application is costly, especially for state-of-the-art Large Language Models (LLMs). Attribution Patching (AtP), a faster gradient-based approximation to Activation Patching, revealing two significant failure modes that result in notable false negatives. To address these, a variant with modifications aimed at mitigating these failure modes while maintaining scalability. Through systematic study AtP outperforms alternative methods for faster activation patching, and AtP* offers further substantial improvement, supplemented by a method to bound the probability of remaining false negatives in AtP* estimates.| [Publication](https://arxiv.org/abs/2403.00745)|
| **LLMs on Tabular Data** -  Recent advancements in large language modeling have enabled extensive exploration of their application in various tabular data modeling tasks. However, there's a lack of comprehensive reviews summarizing key techniques, metrics, datasets, and methodologies in this domain. This survey aims to address this gap by consolidating recent progress, identifying strengths, limitations, and future research directions. It provides valuable insights and references to empower researchers in effectively navigating and tackling challenges in this rapidly evolving field.| [Publication](https://arxiv.org/abs/2402.17944)|
| **LargeLanguageModelsforDataAnnotation:ASurvey** -  explores the potential of advanced Large Language Models (LLMs) like GPT-4 in automating and revolutionizing data annotation, a labor-intensive process crucial for improving machine learning models. It uniquely focuses on LLMs' utility for data annotation, covering methodologies, assessing LLM-generated annotations, and learning strategies. It provides insights into challenges and limitations while aiming to guide researchers and practitioners in leveraging LLMs for enhanced data annotation, fostering advancements in this critical domain.| [Publication](https://arxiv.org/abs/2402.13446)|
|**LoRA+** - demonstrated that the Low Rank Adaptation (LoRA) method, leads to suboptimal fine-tuning for models with large width (embedding dimension) due to the identical learning rate used for adapter matrices A and B. By employing scaling arguments for large width networks, it's illustrated that this uniform learning rate impedes efficient feature learning. To address this, the proposed algorithm, LoRA+, adjusts learning rates for the adapter matrices A and B, resulting in improved performance (1-2% enhancements) and fine-tuning speed (up to âˆ¼ 2X SpeedUp) in extensive experiments, without increased computational cost compared to LoRA. | [Publication](https://arxiv.org/abs/2402.12354)|
|**When is Tree Search Useful for LLM Planning** - explores how large language models (LLMs) approach multi-step problems within a language agent framework, utilizing generators, discriminators, and planning methods. It evaluates two advanced planning methods, iterative correction and tree search, alongside a simpler re-ranking method, across tasks like text-to-SQL parsing and mathematical reasoning. Findings indicate that advanced planning methods require discriminators with at least 90% accuracy to significantly surpass re-ranking. However, current LLMs' discrimination abilities fall short of this threshold, hindering the effectiveness of advanced planning methods. Moreover, when using LLM-based discriminators, balancing accuracy and efficiency proves challenging, as seen with tree search, which, despite being significantly slower, offers minimal performance gains compared to other methods, limiting its practical utility. | [Publication](https://arxiv.org/abs/2402.10890) |
